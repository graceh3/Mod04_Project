{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Selenium and Requests: _Raven+Lily_\n",
    "\n",
    "In this section we will be using selenium and beautiful soup to scrape all of the women's bags from Raven+Lily.\n",
    "First, we will use selenium to get everylink for the bags, and then use Beautiful Soup to get the correct picture,price, product link, and description.\n",
    "\n",
    "Some great sources used to help in this: \n",
    "- [Selenium Website for Locating Elements](https://selenium-python.readthedocs.io/locating-elements.html)\n",
    "- [Stackover flow to Save Images](https://stackoverflow.com/questions/18497840/beautifulsoup-how-to-open-images-and-download-them)\n",
    "- [Get all image src's with specific class](https://stackoverflow.com/questions/8289957/python-2-7-beautiful-soup-img-src-extract)\n",
    "\n",
    "\n",
    "Our steps are as follows: \n",
    "1. [first]()\n",
    "2. [second]()\n",
    "3. [third]()\n",
    "4. [fourth]() \n",
    "\n",
    "#### Import your needed Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create variables for each of the URL\n",
    "\n",
    "We will do this for each of the pages we will be pulling from on Raven+Lily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RL_urls = ['https://ravenandlily.com/collections/backpacks','https://ravenandlily.com/collections/crossbody-bags',\n",
    "        'https://ravenandlily.com/collections/hobos','https://ravenandlily.com/collections/totes',\n",
    "        'https://ravenandlily.com/collections/weekenders']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create our Chrome Driver element and get Links:\n",
    "\n",
    "Here I am using the Chrome Driver Manager because I was facing some issues using the driver, but this resolved all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS_scraper(urls):\n",
    "    img_links_tot = []\n",
    "    desc_tot = []\n",
    "    price_tot = []\n",
    "\n",
    "    for url in urls:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "        match = soup.findAll('div', {'class':'ci'})# find all tags with images\n",
    "        img_links_tot.extend(['https:'+ x.img.findNext('img')['srcset'] for x in match])\n",
    "        match2 = soup.findAll('div', {'class':'product-details'})\n",
    "        desc_tot.extend([x.h3.text for x in match2])\n",
    "        price_tot.extend([x.span.text for x in match2])\n",
    "        \n",
    "    bag_dict = {'link':img_links_tot,'description':desc_tot,'price':price_tot}\n",
    "    return bag_dict\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RL_dict = BS_scraper(RL_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_df = pd.DataFrame(RL_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Clean the product description text:\n",
    "\n",
    "We want to lower case all of the names and get rid of the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_df.description = RL_df.description.apply(lambda x: x.replace('// ','').lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Change the prices to floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_df.price = RL_df.price.apply(lambda x: float(x.replace('$','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_df.to_csv('raven_lily.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop Duplicate Rows based on Column names:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RL_df.drop_duplicates(['description'],inplace=True)\n",
    "RL_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "def save_src_image_apply(url):\n",
    "    start_time = time.time()\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        global c\n",
    "        c += 1\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        image_save_loc = 'raven_lily/'+'rl_'+str(c)+'.jpg'\n",
    "        img.save(image_save_loc, \"JPEG\")\n",
    "        print(\"Saved \"+\"rl_{}\".format(c)+\".png\")\n",
    "        print(c)\n",
    "        return \"rl_\"+ str(c)+\".png\"\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rl_1.png\n",
      "1\n",
      "Saved rl_2.png\n",
      "2\n",
      "Saved rl_3.png\n",
      "3\n",
      "Saved rl_4.png\n",
      "4\n",
      "Saved rl_5.png\n",
      "5\n",
      "Saved rl_6.png\n",
      "6\n",
      "Saved rl_7.png\n",
      "7\n",
      "Saved rl_8.png\n",
      "8\n",
      "Saved rl_9.png\n",
      "9\n",
      "Saved rl_10.png\n",
      "10\n",
      "Saved rl_11.png\n",
      "11\n",
      "Saved rl_12.png\n",
      "12\n",
      "Saved rl_13.png\n",
      "13\n",
      "Saved rl_14.png\n",
      "14\n",
      "Saved rl_15.png\n",
      "15\n",
      "Saved rl_16.png\n",
      "16\n",
      "Saved rl_17.png\n",
      "17\n",
      "Saved rl_18.png\n",
      "18\n",
      "Saved rl_19.png\n",
      "19\n",
      "Saved rl_20.png\n",
      "20\n",
      "Saved rl_21.png\n",
      "21\n",
      "Saved rl_22.png\n",
      "22\n",
      "Saved rl_23.png\n",
      "23\n",
      "Saved rl_24.png\n",
      "24\n",
      "Saved rl_25.png\n",
      "25\n",
      "Saved rl_26.png\n",
      "26\n",
      "Saved rl_27.png\n",
      "27\n",
      "Saved rl_28.png\n",
      "28\n",
      "Saved rl_29.png\n",
      "29\n",
      "Saved rl_30.png\n",
      "30\n",
      "Saved rl_31.png\n",
      "31\n",
      "Saved rl_32.png\n",
      "32\n",
      "Saved rl_33.png\n",
      "33\n",
      "Saved rl_34.png\n",
      "34\n",
      "Saved rl_35.png\n",
      "35\n",
      "Saved rl_36.png\n",
      "36\n",
      "Saved rl_37.png\n",
      "37\n",
      "Saved rl_38.png\n",
      "38\n",
      "Saved rl_39.png\n",
      "39\n",
      "Saved rl_40.png\n",
      "40\n",
      "Saved rl_41.png\n",
      "41\n",
      "Saved rl_42.png\n",
      "42\n",
      "Saved rl_43.png\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "RL_df['img'] = RL_df.link.apply(lambda x: save_src_image_apply(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__After further review of the images it became clear that the correct images did not exist. So, we decided to drop those two rows/products.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_df.drop([16,24],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RL_df.to_csv('data/raven_lily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
